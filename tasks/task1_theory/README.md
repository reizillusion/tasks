#***task1_theory***   

##**theory1**

自瞄系统会对装甲板进行视觉识别，但是有开挂就一定有反制，

无论是识别，控制，云台转动，还是开火射击，都会有一定的延迟，

因此,机器人的自瞄系统必须对装甲板的运动做出预测，

而在“小陀螺”模式下，底盘带着装甲板高速自旋，从而干扰自瞄系统预测的提前量，使对方机器人难以对转动中的装甲板进行锁定，让自瞄系统不能准确瞄准装甲板，

对方机器人的云台也必须持续转动以跟踪或切换目标装甲板，这样就造成视野的晃动和云台自瞄不稳定，让传统自瞄系统难以精准打击装甲板，

因此，在RM赛场上，大多数机器人底盘进行自旋以降低对方机器人对自身装甲板的命中率。

##**theory2**

装甲板上的灯条是亮着的，而比赛场地的环境一般较为复杂，会有很多环境中的光源或反光什么的，

而传统算法，比如基于opencv的图像处理，依赖于画面中目标的形状，颜色，位置等，将画面先处理转化一步，再根据设定的值（亮度或颜色）来识别出灯条所在的位置状态以及数字信息。

若相机曝光太高则会使相机感光过多，获取到的画面整体很亮，类似于FPS游戏中的闪光弹（？）干扰视野，画面中的灯条与周围环境失去细节，，而且原本是红色或蓝色的灯条会被过曝成一片白色，这样就无法区分敌我，甚至灯条的形状也会

变得模糊（大概是会膨胀）。

就像在一个阳光强烈的白天打开手电筒，手电筒的光几乎看不到，而低曝光就相当于把手电筒移到室内，轮廓就会很清晰，

低曝光的相机获取到的画面里，灯条周围环境会在低曝光下被弄得很暗，呈现灰色或黑色，而LED灯条的亮度依然足以被识别，从而减少了周围环境中的画面细节对视觉识别造成的干扰。

因此，机器人上的工业相机一般会适当的降低曝光，以提高识别准确率。

##**theory3**

1.ROS的以linux作为首要支持平台，使用Ubuntu系统可以与ROS无缝衔接。

2.Ubuntu系统的内核与上层系统较为精简，运行时对各类资源的占用较少，可以使资源得到更充分的利用。

3.ubuntu依赖apt包管理器，通过apt-get可以轻松安装各种软件包，开发包，依赖库，在C++程序的开发中具有显著优势。

4.很多深度学习框架和工具对Ubuntu系统有更好的适配和性能，linux对一些设备驱动的支持也更加完善。

##**theory4**

图像分类（文档中叫图像识别）是在整体上对这张图像“是什么”进行判断，给出一个预先定义好的分类标签，

目标检测则是在局部上对图像中各个物体进行识别分类，并且给出物体在图像中的位置和类别。

就像在steam的人机验证里要求我们判断那些照片包含某某物体（x）。

##**theory5**

在理想化的几何光学模型里，光沿直线传播，但是由于凸透镜制造工艺的缺陷等原因，不能保证边缘的光线平行进光轴进入后仍然射向焦点,或者镜头压根就没有平行放置，

所以简单理解为实际上光线在镜头内部是弯曲的，可以说整体上看光并没有沿直线传播， 

相机在将三维的世界转化为二维图像时会产生误差，这就导致原本的图像里直线变成曲线，轮廓发生变形等问题，

虽然人眼几乎观察不到这些细微的畸变，但是对于精确的测量任务，畸变会造成严重的误差，而且会使图像的测量数据变得不可靠，所以有必要对相机进行标定，

通过一些神奇妙妙的数学模型修正二维图像到三维世界的映射关系，从而使对三维世界的精确测量更加精准。

##**theory6**

##**theory7**

以下只是我个人根据群内讨论以及b站一些视频得出的看法：

卡尔曼滤波器用来对物体的位置和运动状态进行预测，

就比如我昨天观测到我的fumo在西土城，而今天看到在沙河，如果用两帧之间的位置差计算fumo的速度就是20km/24h=0.83km/h，

可实际上，我不知道它到底是怎么运动的，可能是坐地铁，或是打车，也可能是直接开隙间传送过去，甚至是绕了地球一圈再回来，所以不能直接判断fumo的运动状态，

实际上，现实中的环境复杂多变，数据本身也不一定靠谱，无论是fumo，还是rm机器人，现实中物体的运动图像大多是一条抖动剧烈的曲线，

而卡尔曼滤波器做的就是过滤掉部分噪声数据，让给出图像更平滑，并且通过根据神奇妙妙的算法估计物体的运动状态，然后合理地预测它未来的状态，

当然，它也只是选择了一个具有一定概率的最优解。













 

